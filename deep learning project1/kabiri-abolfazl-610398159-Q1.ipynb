{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the name  of God\n",
    "\n",
    "#import libraries block\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant definitions\n",
    "bestTrainAccuracy = 0\n",
    "bestValueAccuracy = 0\n",
    "goalLearningRate = 0\n",
    "bestReg = 0\n",
    "learningRatesList = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "regList= [0.001 ,0.01 ,0.1, 0.5, 0.75, 1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions definitions block\n",
    "\n",
    "#calculate number of missClassified Data \n",
    "def NumberOfMissClassifiedData(y, p) :\n",
    "    sum = 0\n",
    "    for i in range(len(y)) :\n",
    "        if y[i] != p[i] :\n",
    "            sum+=1\n",
    "    return sum \n",
    "\n",
    "#logistic model definition\n",
    "def model(rate, reg, maxIteration, tolerance, X, Y) :\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    W = np.zeros(X.shape[1])\n",
    "    \n",
    "    #in this loop we train the model by checking tolerance\n",
    "    for i in range(maxIteration) :\n",
    "        err = (1 / (1 + np.exp(-(X @ W)))) - Y\n",
    "        gradian_delta = (((1 / reg) * (np.transpose(X) @ err)) + np.sum(W))\n",
    "        if np.all(abs(gradian_delta) >= tolerance):\n",
    "            W -= rate * gradian_delta / (X.shape[1])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId : 0 - Survived : 0 - Pclass : 0 - Name : 0 - Sex : 0 - Age : 177 - SibSp : 0 - Parch : 0 - Ticket : 0 - Fare : 0 - Cabin : 687 - Embarked : 2 - \n",
      "\n",
      "\n",
      "\n",
      "PassengerId  :  0 - Pclass  :  0 - Name  :  0 - Sex  :  0 - Age  :  86 - SibSp  :  0 - Parch  :  0 - Ticket  :  0 - Fare  :  1 - Cabin  :  327 - Embarked  :  0 - "
     ]
    }
   ],
   "source": [
    "#reading test and train files\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#now we display number of missing data in each columns of train data and in test data\n",
    "for i in list(train_data.columns) :\n",
    "    print(i, \":\", sum(train_data[i].isnull()), end=\" - \")\n",
    "print(\"\\n\\n\\n\")\n",
    "for i in list(test_data.columns) :\n",
    "    print(i, \" : \", sum(test_data[i].isnull()), end=\" - \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 7)\n",
      "(891, 7)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "#in this section we replace missing datas and convert non numerical features to numerical features\n",
    "#and convert to numrical features and replace them empty places\n",
    "\n",
    "#fill missing data of age column with median of each sex of each pclass for test and train data\n",
    "train_data['Age'] = train_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda i: i.fillna(i.median()))\n",
    "test_data['Age'] = test_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda i: i.fillna(i.median()))\n",
    "\n",
    "#convert non numerical features to numerical features\n",
    "#convert values of Sex features to 0(male),1(female)\n",
    "#convert values of Embarked features to 0(C),1(Q),2(S)\n",
    "train_data['Sex'].replace([\"male\", \"female\"], [0.0,1.0], inplace=True)\n",
    "test_data['Sex'].replace([\"male\", \"female\"], [0.0,1.0], inplace=True)\n",
    "train_data[\"Embarked\"].replace([\"C\", \"Q\", \"S\"], [0.0, 1.0, 2.0], inplace=True)\n",
    "test_data[\"Embarked\"].replace([\"C\", \"Q\", \"S\"], [0.0, 1.0, 2.0], inplace=True)\n",
    "\n",
    "#we delete Inappropriate features from train and test data\n",
    "train_data = train_data.drop([\"Cabin\", \"Name\", \"PassengerId\", \"Ticket\"], axis=1)\n",
    "test_data = test_data.drop([\"Cabin\", \"Name\", \"PassengerId\", \"Ticket\"], axis=1)\n",
    "\n",
    "#then we replace missing values by median of column\n",
    "train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(train_data[\"Embarked\"].median())\n",
    "test_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data[\"Fare\"].median())\n",
    "\n",
    "#initialize X_text, X_train, Y_train and print its shape\n",
    "X_train = train_data[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]\n",
    "Y_train = train_data[[\"Survived\"]]\n",
    "X_test = test_data.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n",
      "[[ 0.82737724 -0.73769513 -0.53489116  0.43279337 -0.47367361 -0.50244517\n",
      "   0.58595414]\n",
      " [-1.56610693  1.35557354  0.66839176  0.43279337 -0.47367361  0.78684529\n",
      "  -1.9423032 ]\n",
      " [ 0.82737724  1.35557354 -0.23407043 -0.4745452  -0.47367361 -0.48885426\n",
      "   0.58595414]\n",
      " [-1.56610693  1.35557354  0.44277621  0.43279337 -0.47367361  0.42073024\n",
      "   0.58595414]\n",
      " [ 0.82737724 -0.73769513  0.44277621 -0.4745452  -0.47367361 -0.48633742\n",
      "   0.58595414]]\n",
      "(418, 7)\n",
      "[[ 0.87348191 -0.75592895  0.39945123 -0.49947002 -0.4002477  -0.49741333\n",
      "  -0.47091535]\n",
      " [ 0.87348191  1.32287566  1.35927311  0.61699237 -0.4002477  -0.51227801\n",
      "   0.70076689]\n",
      " [-0.31581919 -0.75592895  2.51105936 -0.49947002 -0.4002477  -0.46410047\n",
      "  -0.47091535]\n",
      " [ 0.87348191 -0.75592895 -0.1764419  -0.49947002 -0.4002477  -0.48247516\n",
      "   0.70076689]\n",
      " [ 0.87348191  1.32287566 -0.56037065  0.61699237  0.61989583 -0.4174915\n",
      "   0.70076689]]\n"
     ]
    }
   ],
   "source": [
    "#first normalize X_train and X_test\n",
    "#then print them shapes and five first elements\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / (np.sqrt(np.var(X_train, axis=0)))\n",
    "print(X_train.shape)\n",
    "print(X_train[:5])\n",
    "\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / (np.sqrt(np.var(X_test, axis=0)))\n",
    "print(X_test.shape)\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in exp\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in exp\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate =  0.1 regularization constant =  0.001\n",
      "accuarcy in train:  72.28330995792426 accuarcy in validation data:  72.53932584269663 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  0.01\n",
      "accuarcy in train:  71.89621318373071 accuarcy in validation data:  71.56179775280899 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  0.1\n",
      "accuarcy in train:  72.8835904628331 accuarcy in validation data:  72.21348314606742 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  0.5\n",
      "accuarcy in train:  74.49649368863956 accuarcy in validation data:  73.92134831460675 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  0.75\n",
      "accuarcy in train:  75.50070126227209 accuarcy in validation data:  74.97752808988764 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  1\n",
      "accuarcy in train:  76.44319775596072 accuarcy in validation data:  76.11235955056179 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  5\n",
      "accuarcy in train:  80.08415147265077 accuarcy in validation data:  79.58426966292134 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  10\n",
      "accuarcy in train:  79.76157082748948 accuarcy in validation data:  79.41573033707866 \n",
      "\n",
      "\n",
      "learning rate =  0.1 regularization constant =  50\n",
      "accuarcy in train:  78.96213183730715 accuarcy in validation data:  78.29213483146067 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  0.001\n",
      "accuarcy in train:  71.7952314165498 accuarcy in validation data:  71.61797752808988 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  0.01\n",
      "accuarcy in train:  72.12903225806451 accuarcy in validation data:  71.59550561797752 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  0.1\n",
      "accuarcy in train:  72.33380084151473 accuarcy in validation data:  71.0 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  0.5\n",
      "accuarcy in train:  72.68162692847125 accuarcy in validation data:  71.65168539325842 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  0.75\n",
      "accuarcy in train:  71.90182328190744 accuarcy in validation data:  71.2247191011236 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  1\n",
      "accuarcy in train:  74.22720897615709 accuarcy in validation data:  73.86516853932585 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  5\n",
      "accuarcy in train:  80.11781206171108 accuarcy in validation data:  79.69662921348315 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  10\n",
      "accuarcy in train:  79.84291725105189 accuarcy in validation data:  79.4494382022472 \n",
      "\n",
      "\n",
      "learning rate =  0.2 regularization constant =  50\n",
      "accuarcy in train:  78.94810659186535 accuarcy in validation data:  78.69662921348315 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  0.001\n",
      "accuarcy in train:  71.17251051893409 accuarcy in validation data:  71.41573033707866 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  0.01\n",
      "accuarcy in train:  72.20196353436185 accuarcy in validation data:  72.01123595505618 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  0.1\n",
      "accuarcy in train:  71.83450210378682 accuarcy in validation data:  72.0 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  0.5\n",
      "accuarcy in train:  69.60729312762973 accuarcy in validation data:  68.80898876404494 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  0.75\n",
      "accuarcy in train:  71.05189340813465 accuarcy in validation data:  70.57303370786516 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  1\n",
      "accuarcy in train:  72.05610098176717 accuarcy in validation data:  71.70786516853933 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  5\n",
      "accuarcy in train:  80.21598877980364 accuarcy in validation data:  79.71910112359551 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  10\n",
      "accuarcy in train:  79.781206171108 accuarcy in validation data:  79.53932584269663 \n",
      "\n",
      "\n",
      "learning rate =  0.3 regularization constant =  50\n",
      "accuarcy in train:  78.98176718092566 accuarcy in validation data:  78.58426966292134 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  0.001\n",
      "accuarcy in train:  72.08415147265077 accuarcy in validation data:  71.91011235955057 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  0.01\n",
      "accuarcy in train:  71.87096774193549 accuarcy in validation data:  71.26966292134831 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  0.1\n",
      "accuarcy in train:  71.32678821879384 accuarcy in validation data:  70.32584269662922 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  0.5\n",
      "accuarcy in train:  72.08415147265077 accuarcy in validation data:  71.33707865168539 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  0.75\n",
      "accuarcy in train:  71.65217391304347 accuarcy in validation data:  71.0561797752809 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  1\n",
      "accuarcy in train:  71.17531556802244 accuarcy in validation data:  70.85393258426967 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  5\n",
      "accuarcy in train:  80.01683029453015 accuarcy in validation data:  79.40449438202248 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  10\n",
      "accuarcy in train:  79.8288920056101 accuarcy in validation data:  79.48314606741573 \n",
      "\n",
      "\n",
      "learning rate =  0.4 regularization constant =  50\n",
      "accuarcy in train:  78.96213183730715 accuarcy in validation data:  78.7752808988764 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  0.001\n",
      "accuarcy in train:  69.57363253856943 accuarcy in validation data:  69.59550561797754 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  0.01\n",
      "accuarcy in train:  71.70827489481066 accuarcy in validation data:  71.69662921348313 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  0.1\n",
      "accuarcy in train:  72.28050490883591 accuarcy in validation data:  71.50561797752809 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  0.5\n",
      "accuarcy in train:  71.53997194950912 accuarcy in validation data:  70.3932584269663 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  0.75\n",
      "accuarcy in train:  70.41234221598877 accuarcy in validation data:  69.74157303370787 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  1\n",
      "accuarcy in train:  70.17671809256662 accuarcy in validation data:  69.61797752808988 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  5\n",
      "accuarcy in train:  75.91304347826087 accuarcy in validation data:  75.42696629213484 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  10\n",
      "accuarcy in train:  79.84572230014025 accuarcy in validation data:  79.40449438202248 \n",
      "\n",
      "\n",
      "learning rate =  0.5 regularization constant =  50\n",
      "accuarcy in train:  79.07713884992987 accuarcy in validation data:  78.74157303370787 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  0.001\n",
      "accuarcy in train:  72.54978962131837 accuarcy in validation data:  71.07865168539325 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  0.01\n",
      "accuarcy in train:  69.81767180925667 accuarcy in validation data:  68.38202247191012 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  0.1\n",
      "accuarcy in train:  72.50490883590463 accuarcy in validation data:  71.95505617977528 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  0.5\n",
      "accuarcy in train:  69.0070126227209 accuarcy in validation data:  68.3370786516854 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  0.75\n",
      "accuarcy in train:  69.24263674614306 accuarcy in validation data:  68.73033707865169 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  1\n",
      "accuarcy in train:  68.68723702664796 accuarcy in validation data:  68.3370786516854 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  5\n",
      "accuarcy in train:  74.82748948106592 accuarcy in validation data:  74.30337078651685 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  10\n",
      "accuarcy in train:  79.86816269284712 accuarcy in validation data:  79.28089887640449 \n",
      "\n",
      "\n",
      "learning rate =  0.6 regularization constant =  50\n",
      "accuarcy in train:  79.0210378681627 accuarcy in validation data:  78.80898876404494 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  0.001\n",
      "accuarcy in train:  68.24684431977559 accuarcy in validation data:  66.92134831460675 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  0.01\n",
      "accuarcy in train:  69.0687237026648 accuarcy in validation data:  67.62921348314606 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  0.1\n",
      "accuarcy in train:  68.3899018232819 accuarcy in validation data:  68.01123595505618 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  0.5\n",
      "accuarcy in train:  69.19495091164096 accuarcy in validation data:  68.23595505617978 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  0.75\n",
      "accuarcy in train:  69.50350631136044 accuarcy in validation data:  68.59550561797752 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  1\n",
      "accuarcy in train:  66.52173913043478 accuarcy in validation data:  66.06741573033707 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  5\n",
      "accuarcy in train:  73.41374474053296 accuarcy in validation data:  72.79775280898876 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  10\n",
      "accuarcy in train:  80.40953716690042 accuarcy in validation data:  79.71910112359551 \n",
      "\n",
      "\n",
      "learning rate =  0.7 regularization constant =  50\n",
      "accuarcy in train:  79.09677419354838 accuarcy in validation data:  78.76404494382022 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  0.001\n",
      "accuarcy in train:  66.87237026647966 accuarcy in validation data:  66.29213483146067 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  0.01\n",
      "accuarcy in train:  67.09677419354838 accuarcy in validation data:  65.96629213483146 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  0.1\n",
      "accuarcy in train:  68.28611500701263 accuarcy in validation data:  68.23595505617978 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  0.5\n",
      "accuarcy in train:  66.95932678821879 accuarcy in validation data:  65.97752808988764 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  0.75\n",
      "accuarcy in train:  68.34502103786816 accuarcy in validation data:  68.0 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  1\n",
      "accuarcy in train:  68.03366058906032 accuarcy in validation data:  67.56179775280899 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  5\n",
      "accuarcy in train:  71.50070126227209 accuarcy in validation data:  71.14606741573033 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  10\n",
      "accuarcy in train:  78.34782608695653 accuarcy in validation data:  78.03370786516854 \n",
      "\n",
      "\n",
      "learning rate =  0.8 regularization constant =  50\n",
      "accuarcy in train:  79.01262272089761 accuarcy in validation data:  78.74157303370787 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  0.001\n",
      "accuarcy in train:  64.41234221598879 accuarcy in validation data:  63.82022471910112 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  0.01\n",
      "accuarcy in train:  67.19495091164094 accuarcy in validation data:  66.7191011235955 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  0.1\n",
      "accuarcy in train:  64.64796633941094 accuarcy in validation data:  64.20224719101124 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  0.5\n",
      "accuarcy in train:  67.53716690042076 accuarcy in validation data:  67.10112359550561 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  0.75\n",
      "accuarcy in train:  66.56661991584852 accuarcy in validation data:  66.0561797752809 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  1\n",
      "accuarcy in train:  67.75596072931276 accuarcy in validation data:  67.15730337078651 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  5\n",
      "accuarcy in train:  69.99719495091165 accuarcy in validation data:  69.69662921348315 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  10\n",
      "accuarcy in train:  76.35904628330997 accuarcy in validation data:  76.06741573033707 \n",
      "\n",
      "\n",
      "learning rate =  0.9 regularization constant =  50\n",
      "accuarcy in train:  79.06591865357643 accuarcy in validation data:  78.8314606741573 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "train_data = train_data.to_numpy()\n",
    "for rate in learningRatesList :\n",
    "    for reg in regList :\n",
    "        #MCV : miss Classififed Values List\n",
    "        #MCL : miss Classififed Train List\n",
    "        MCV = []\n",
    "        MCL = []\n",
    "        for i in range(10) :\n",
    "            np.random.shuffle(train_data)\n",
    "            X_train = train_data[: , 1 : ]\n",
    "            Y_train = train_data[: , 0]\n",
    "            X_train = (X_train - np.mean(X_train, axis=0)) / (np.sqrt(np.var(X_train, axis=0)))\n",
    "            #because 5 fold cross validation\n",
    "            N = X_train.shape[0]//5\n",
    "            for i in range(5) :\n",
    "                xt = np.concatenate((X_train[ : i * N], X_train[(i+1) * N : ]), axis=0)\n",
    "                xv = X_train[i * N : (i + 1) * N] \n",
    "                yt = np.concatenate((Y_train[ : i * N], Y_train[(i+1) * N : ]), axis=0)\n",
    "                yv = Y_train[i * N : (i + 1) * N]\n",
    "                weights = model(rate, reg, 500, 0.001, xt, yt)\n",
    "                #calculateing missClassified value and missClassified List\n",
    "                MCL.append(NumberOfMissClassifiedData(yt, np.round(1 / (1 + np.exp(-((xt @ weights[1:]) + weights[0]))))))\n",
    "                MCV.append(NumberOfMissClassifiedData(yv, np.round(1 / (1 + np.exp(-((xv @ weights[1:]) + weights[0]))))))\n",
    "\n",
    "        print(\"learning rate = \", rate , \"regularization constant = \", reg)\n",
    "        #calculate train and value Accuraicy\n",
    "        trainAcc =100.0 - ((sum(MCL)/len(MCL))/713)*100\n",
    "        valueAcc =100.0 - ((sum(MCV)/len(MCV))/178)*100\n",
    "        print(\"accuarcy in train: \", trainAcc, \"accuarcy in validation data: \", valueAcc, \"\\n\\n\")\n",
    "        if trainAcc > bestTrainAccuracy and valueAcc > bestValueAccuracy :\n",
    "            bestTrainAccuracy = trainAcc\n",
    "            bestValueAccuracy = valueAcc\n",
    "            goalLearningRate = rate\n",
    "            bestReg = reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuarcy on train :  80.21598877980364\n",
      "best accuarcy on validation :  79.71910112359551\n",
      "learning rate :  0.3\n",
      "regularization constant :  5\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"best accuarcy on train : \", bestTrainAccuracy)\n",
    "print(\"best accuarcy on validation : \", bestValueAccuracy)\n",
    "print(\"learning rate : \", goalLearningRate)\n",
    "print(\"regularization constant : \", bestReg)\n",
    "\n",
    "#use from model\n",
    "weights = model(goalLearningRate, bestReg, 500, 0.001, X_train, Y_train)\n",
    "#print predicate y\n",
    "print(np.round(1 / (1 + np.exp(-((X_test @ weights[1:]) + weights[0])))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e1c24d82d5361d9168360964fb5a5b1aa431ab40d0bc4d411808c4ada5f51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
